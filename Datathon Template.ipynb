{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8b43f631f876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuberRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import HuberRegressor, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gain insight on the dataset\n",
    "def eda_pipeline(df):\n",
    "    \n",
    "    # Basic Information\n",
    "    print(\"Dataset Shape:\", df.shape)\n",
    "    print(\"\\nColumn Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Missing Values\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percent = (missing_values / len(df)) * 100\n",
    "    missing_info = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percent})\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(missing_info[missing_info['Missing Values'] > 0])\n",
    "    \n",
    "    # Categorical Cardinality\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 1:\n",
    "        print(\"\\nCategorical Columns:\", categorical_cols)\n",
    "        print(\"\\nCategorical Cardinality:\")\n",
    "        for col in categorical_cols:\n",
    "            cardinality = len(df[col].unique())\n",
    "            print(f\"{col}: {cardinality} unique values\")\n",
    "    else:\n",
    "        print(\"We do not have any categorical feature\\nCheck for the categoricals with type of string\")\n",
    "        \n",
    "    # Numerical Summary\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numerical_cols) > 1:\n",
    "        print(\"\\nNumerical Columns:\", numerical_cols)\n",
    "        print(\"\\nNumerical Summary:\")\n",
    "        print(df[numerical_cols].describe())\n",
    "        \n",
    "        #Outlier examination with IQR and BoxPlot\n",
    "        outliers = {}\n",
    "        for col in numerical_cols:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers[col] = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "            print(f'Outliers of {col}: {outliers[col]}')\n",
    "                  \n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.boxplot(data=df, y=col)\n",
    "            plt.title(f'Box Plot of {col}')\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"We do not have any numeric feature\")\n",
    "    \n",
    "    # Distribution Plots for Numerical Columns\n",
    "    for col in numerical_cols:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.histplot(data=df, x=col, kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.show()\n",
    "        \n",
    "    # Correlation Matrix for Numerical Columns\n",
    "    correlation_matrix = df[numerical_cols].corr()\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation for the models not to have any error. fillna, encode categoricals, scale numericals\n",
    "def baseline_prep(df, target, classification = None, scaler = None):\n",
    "    numerical_cols = df.select_dtypes(exclude=['object']).columns\n",
    "    df[numerical_cols] = df[numerical_cols].fillna(df[numerical_cols].mean())\n",
    "    \n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])\n",
    "\n",
    "    if len(categorical_cols) > 0:\n",
    "        if classification:\n",
    "            label_encoder = LabelEncoder()\n",
    "            df[target] = label_encoder.fit_transform(df[target])\n",
    "        df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "        \n",
    "    if len(numerical_cols) > 0:\n",
    "        if scaler is None:\n",
    "            scaler = StandardScaler()\n",
    "        df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Classifier models in diversed characteristics with evaluating them using accuracy, f1, precision, recall\n",
    "def baseline_clf(df, target_col, test_size=0.2, random_state=42):\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    global X_train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(),\n",
    "        \"XGBoost\": XGBClassifier(),\n",
    "        \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "        \"Random Forest\": RandomForestClassifier()\n",
    "    }\n",
    "\n",
    "    global tuned_models_clf\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        if model_name == \"Logistic Regression\":\n",
    "            param_grid = {\n",
    "                'C': [0.01, 0.1, 1, 10],\n",
    "                'penalty': ['l1', 'l2']\n",
    "            }\n",
    "            grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            tuned_models_clf[model_name] = grid_search.best_estimator_\n",
    "            \n",
    "        elif model_name == \"XGBoost\":\n",
    "            param_grid = {\n",
    "                'n_estimators': [100, 200, 300],\n",
    "                'max_depth': [3, 4, 5],\n",
    "                'learning_rate': [0.01, 0.1, 0.2]\n",
    "            }\n",
    "            grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            tuned_models_clf[model_name] = grid_search.best_estimator_\n",
    "            \n",
    "        else:\n",
    "            tuned_models_clf[model_name] = model\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in tuned_models_clf.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        results[model_name] = {\"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
    "\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Regression models in diversed characteristics with evaluating them using MSE,MAE,MAPE,R^2\n",
    "def baseline_reg(df, target, test_size = 0.2, random_state = 42):\n",
    "    def mean_abs_percentage_error(y_true, y_pred):\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    X = df.drop(target, axis = 1)\n",
    "    y = df[target]\n",
    "    \n",
    "    global X_train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = random_state)\n",
    "    \n",
    "    models = {\n",
    "        \"Random Forest\": RandomForestRegressor(),\n",
    "        \"XGBoost\": XGBRegressor(),\n",
    "        \"KNN\": KNeighborsRegressor(),\n",
    "        \"Huber Regressor\": HuberRegressor()\n",
    "    }\n",
    "    \n",
    "    global tuned_models_reg\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        if model_name == \"Random Forest\":\n",
    "            param_grid = {\n",
    "                'n_estimators': [100,200,300],\n",
    "                'max_depth': [10,20,30],\n",
    "                'min_samples_split': [2,5,10],\n",
    "                'min_samples_leaf': [1,2,4]\n",
    "            }\n",
    "            grid_search = GridSearchCV(model, param_grid, cv = 5, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            tuned_models_reg[model_name] = grid_search.best_estimator_\n",
    "            \n",
    "        elif model_name == \"XGBoost\":\n",
    "            param_grid = {\n",
    "                'n_estimators': [100,200,300],\n",
    "                'max_depth': [3,4,5],\n",
    "                'learning_rate': [0.01,0.1,0.2]\n",
    "            }\n",
    "            \n",
    "        elif model_name == \"Huber Regressor\":\n",
    "            tuned_models_reg[model_name] = model\n",
    "            \n",
    "        else:\n",
    "            tuned_models_reg[model_name] = model\n",
    "            \n",
    "    results = {}\n",
    "    \n",
    "    for model_name, model in tuned_models_reg.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_abs_percentage_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        results[model_name] = {\"MSE\": mse, \"MAE\": mae, \"MAPE\": mape, \"R-squared\": r2}\n",
    "        \n",
    "    print(results)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance plot.For instance: the case is classification, baseline_feature_importance(tuned_models_clf, X_train)\n",
    "def baseline_feature_importance(models, X_train):\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importances = model.feature_importances_\n",
    "        elif hasattr(model, 'coef_'):\n",
    "            feature_importances = model.coef_[0]\n",
    "        else:\n",
    "            print(f\"Built-in feature importance not available for model: {model_name}\")\n",
    "            continue\n",
    "\n",
    "        feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize = (10, 6))\n",
    "        sns.barplot(x = 'Importance', y = 'Feature', data = feature_importance_df)\n",
    "        plt.title(f\"Feature Importance for {model_name}\")\n",
    "        plt.xlabel(\"Importance Score\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
